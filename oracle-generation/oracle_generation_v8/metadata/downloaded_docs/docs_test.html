<!doctype html><html lang=en><meta charset=utf-8><title>Testing · V8</title><meta content="width=device-width" name=viewport><meta content="dark light" name=color-scheme><link href=/_css/main.css rel=stylesheet><link href=/favicon.png rel=icon sizes=48x48><link href=/.webmanifest rel=manifest><meta content=#4285F4 name=theme-color><link href=/blog.atom rel=alternate title="V8 Atom feed" type=application/atom+xml><link href=/features.atom rel=alternate title="V8 JS/Wasm features Atom feed" type=application/atom+xml><script>document.documentElement.className+=" js"</script><meta content="This document explains the testing framework that is part of the V8 repository." name=description><header id=header><h1><a href=/ >V8</a></h1><a href=#navigation-toggle id=nav-toggle>Show navigation</a><nav><ul><li><a href=/ >Home</a><li><a href=/blog>Blog</a><li class=current><a href=/docs>Docs</a><li><a href=/tools>Tools</a><li><a href=/features title="JavaScript and WebAssembly language features">JS/Wasm features</a><li><a href=/grant>Research</a></ul></nav></header><main id=main><h1>Testing</h1><p>V8 includes a test framework that allows you to test the engine. The framework lets you run both our own test suites that are included with the source code and others, such as <a href=https://github.com/tc39/test262>the Test262 test suite</a>.<h2 id=running-the-v8-tests tabindex=-1>Running the V8 tests <a href=#running-the-v8-tests class=bookmark>#</a></h2><p><a href=/docs/build-gn#gm>Using <code>gm</code></a>, you can simply append <code>.check</code> to any build target to have tests run for it, e.g.<pre class=language-bash><code class=language-bash>gm x64.release.check<br>gm x64.optdebug.check  <span class="token comment"># recommended: reasonably fast, with DCHECKs.</span><br>gm ia32.check<br>gm release.check<br>gm check  <span class="token comment"># builds and tests all default platforms</span></code></pre><p><code>gm</code> automatically builds any required targets before running the tests. You can also limit the tests to be run:<pre class=language-bash><code class=language-bash>gm x64.release test262<br>gm x64.debug mjsunit/regress/regress-123</code></pre><p>If you have already built V8, you can run the tests manually:<pre class=language-bash><code class=language-bash>tools/run-tests.py <span class="token parameter variable">--outdir</span><span class="token operator">=</span>out/ia32.release</code></pre><p>Again, you can specify which tests to run:<pre class=language-bash><code class=language-bash>tools/run-tests.py <span class="token parameter variable">--outdir</span><span class="token operator">=</span>ia32.release cctest/test-heap/SymbolTable/* mjsunit/delete-in-eval</code></pre><p>Run the script with <code>--help</code> to find out about its other options.<h2 id=running-more-tests tabindex=-1>Running more tests <a href=#running-more-tests class=bookmark>#</a></h2><p>The default set of tests to be run does not include all available tests. You can specify additional test suites on the command line of either <code>gm</code> or <code>run-tests.py</code>:<ul><li><code>benchmarks</code> (just for correctness; does not produce benchmark results!)<li><code>mozilla</code><li><code>test262</code><li><code>webkit</code></ul><h2 id=running-microbenchmarks tabindex=-1>Running microbenchmarks <a href=#running-microbenchmarks class=bookmark>#</a></h2><p>Under <code>test/js-perf-test</code> we have microbenchmarks to track feature performance. There is a special runner for these: <code>tools/run_perf.py</code>. Run them like:<pre class=language-bash><code class=language-bash>tools/run_perf.py <span class="token parameter variable">--arch</span> x64 --binary-override-path out/x64.release/d8 test/js-perf-test/JSTests.json</code></pre><p>If you don’t want to run all the <code>JSTests</code>, you can provide a <code>filter</code> argument:<pre class=language-bash><code class=language-bash>tools/run_perf.py <span class="token parameter variable">--arch</span> x64 --binary-override-path out/x64.release/d8 <span class="token parameter variable">--filter</span> JSTests/TypedArrays test/js-perf-test/JSTests.json</code></pre><h2 id=updating-the-inspector-test-expectations tabindex=-1>Updating the inspector test expectations <a href=#updating-the-inspector-test-expectations class=bookmark>#</a></h2><p>After updating your test, you might need to regenerate the expectations file for it. You can achieve this by running:<pre class=language-bash><code class=language-bash>tools/run-tests.py --regenerate-expected-files <span class="token parameter variable">--outdir</span><span class="token operator">=</span>ia32.release inspector/debugger/set-instrumentation-breakpoint</code></pre><p>This can also be useful if you want to find out how the output of your test changed. First regenerate the expected file using the command above, then check the diff with:<pre class=language-bash><code class=language-bash><span class="token function">git</span> <span class="token function">diff</span></code></pre><h2 id=updating-the-bytecode-expectations-(rebaselining) tabindex=-1>Updating the bytecode expectations (rebaselining) <a href=#updating-the-bytecode-expectations-(rebaselining) class=bookmark>#</a></h2><p>Sometimes the bytecode expectations may change resulting in <code>cctest</code> failures. To update the golden files, build <code>test/cctest/generate-bytecode-expectations</code> by running:<pre class=language-bash><code class=language-bash>gm x64.release generate-bytecode-expectations</code></pre><p>…and then updating the default set of inputs by passing the <code>--rebaseline</code> flag to the generated binary:<pre class=language-bash><code class=language-bash>out/x64.release/generate-bytecode-expectations <span class="token parameter variable">--rebaseline</span></code></pre><p>The updated goldens are now available in <code>test/cctest/interpreter/bytecode_expectations/</code>.<h2 id=adding-a-new-bytecode-expectations-test tabindex=-1>Adding a new bytecode expectations test <a href=#adding-a-new-bytecode-expectations-test class=bookmark>#</a></h2><ol><li><p>Add a new test case to <code>cctest/interpreter/test-bytecode-generator.cc</code> and specify a golden file with the same test name.<li><p>Build <code>generate-bytecode-expectations</code>:<pre class=language-bash><code class=language-bash>gm x64.release generate-bytecode-expectations</code></pre><li><p>Run<pre class=language-bash><code class=language-bash>out/x64.release/generate-bytecode-expectations --raw-js testcase.js <span class="token parameter variable">--output</span><span class="token operator">=</span>test/cctest/interpreter/bytecode-expectations/testname.golden</code></pre><p>where <code>testcase.js</code> contains the JavaScript test case that was added to <code>test-bytecode-generator.cc</code> and <code>testname</code> is the name of the test defined in <code>test-bytecode-generator.cc</code>.</ol></main><footer id=footer><div><nav><a href=/logo>Branding</a> <a href=/terms>Terms</a> <a href=https://policies.google.com/privacy>Privacy</a> <a href=https://twitter.com/v8js rel="me nofollow">Twitter</a> <a href=https://github.com/v8/v8.dev/tree/main/./src/docs/test.md rel=nofollow>Edit this page on GitHub</a></nav><dark-mode-toggle dark="Light Mode" light="Dark Mode" permanent></dark-mode-toggle></div><p><small>Except as otherwise noted, any code samples from the V8 project are licensed under <a href=https://chromium.googlesource.com/v8/v8.git/+/main/LICENSE>V8’s BSD-style license</a>. Other content on this page is licensed under <a href=https://creativecommons.org/licenses/by/3.0/ >the Creative Commons Attribution 3.0 License</a>. For details, see <a href=/terms#site-policies>our site policies</a>.</small></footer><script src=/_js/dark-mode-toggle.mjs type=module></script><script src=/_js/main.js></script>